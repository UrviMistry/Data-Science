{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Final Project Code: Grocery Sales Prediction </center>\n",
    "## <center> Big Data Analytics </center>\n",
    "## <center> By: Urvi Mistry | Purva Kedari | Trisha Chakraborty </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using for Grocery Sales Prediction was retreived from Kaggle.\n",
    "This dataset is a part of the Kaggle Competition named, CorporaciÃ³n Favorita Grocery Sales Forecasting. The goal of this project is to predict Sales based on Day and Month as well as Item number and Month.\n",
    "\n",
    "Link to the Kaggle: https://www.kaggle.com/c/favorita-grocery-sales-forecasting/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline, evaluation\n",
    "\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "from pyspark import sql\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing csv files\n",
    "test_df = spark.read.csv(\"test.csv\", header=True).limit(120000)\n",
    "train_df = spark.read.csv(\"train.csv\", header=True)\n",
    "items_df = spark.read.csv(\"items.csv\", header=True)\n",
    "stores_df = spark.read.csv(\"stores.csv\", header=True)\n",
    "holidays_df = spark.read.csv(\"holidays_events.csv\", header=True)\n",
    "transactions_df = spark.read.csv(\"transactions.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming some columns and dropping some unnecessary columns\n",
    "stores_df = stores_df.withColumnRenamed(\"type\",\"store_type\")\n",
    "holidays_df = holidays_df.withColumnRenamed(\"type\",\"holiday_type\")\n",
    "holidays_df = holidays_df.drop('description','transferred')\n",
    "train_df = train_df.drop('id')\n",
    "test_df = test_df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As all the numerical columns were wrongly converted to String from csv files. Thus, converting those columns back to integer\n",
    "train_df = train_df.withColumn(\"store_nbr\", train_df[\"store_nbr\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"item_nbr\", train_df[\"item_nbr\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"unit_sales\", train_df[\"unit_sales\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"store_nbr\", test_df[\"store_nbr\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"item_nbr\", test_df[\"item_nbr\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"item_nbr\",items_df[\"item_nbr\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"class\",items_df[\"class\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"perishable\",items_df[\"perishable\"].cast(IntegerType()))\n",
    "stores_df = stores_df.withColumn(\"store_nbr\",stores_df[\"store_nbr\"].cast(IntegerType()))\n",
    "stores_df = stores_df.withColumn(\"cluster\",stores_df[\"cluster\"].cast(IntegerType()))\n",
    "transactions_df = transactions_df.withColumn(\"store_nbr\",transactions_df[\"store_nbr\"].cast(IntegerType()))\n",
    "transactions_df = transactions_df.withColumn(\"transactions\",transactions_df[\"transactions\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(False, 0.001,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sample(False, 0.1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the dataframe into train_df\n",
    "train_df = train_df.join(stores_df, on = 'store_nbr', how = 'left')\n",
    "train_df = train_df.join(items_df, on = 'item_nbr', how = 'left')\n",
    "train_df = train_df.join(holidays_df, on = 'date', how = 'left')\n",
    "train_df = train_df.join(transactions_df, on = ['store_nbr','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the dataframes into test_df\n",
    "test_df = test_df.join(stores_df, on = 'store_nbr', how = 'left')\n",
    "test_df = test_df.join(items_df, on = 'item_nbr', how = 'left')\n",
    "test_df = test_df.join(holidays_df, on = 'date', how = 'left')\n",
    "test_df = test_df.join(transactions_df, on = ['store_nbr','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the csv files generates NULL values. For example, when holidays.csv was joined with train.csv there were NULL values generated, as train.csv contained all the dates and not all those dates were a holiday. \n",
    "\n",
    "Thus, we replaced these NULL values with some value. For example, dates which didn't have a holiday were replaced with NO Holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling null values with some value\n",
    "train_df = train_df.fillna({'onpromotion':'False','holiday_type':'No Holiday','locale':'Not Applicable',\\\n",
    "                            'locale_name':'Not Applicable', 'transactions':1884})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.fillna({'onpromotion':'False','holiday_type':'No Holiday','locale':'Not Applicable',\\\n",
    "                            'locale_name':'Not Applicable', 'transactions':1884})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset had some errored values where the unit_sales column contained values less than 0 which was definitely errored. As these errored rows were very few (about 3), we decided to drop them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = (train_df.filter(train_df.unit_sales > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (train_df.filter(train_df.unit_sales > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Date column into Day, Month and Year for performing Monthly analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pyspark.sql.functions.split(train_df['date'], '-')     \n",
    "train_df= train_df.withColumn('Year', split_date.getItem(0))\n",
    "train_df= train_df.withColumn('Month', split_date.getItem(1))\n",
    "train_df= train_df.withColumn('Day', split_date.getItem(2))\n",
    "train_df = train_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pyspark.sql.functions.split(test_df['date'], '-')     \n",
    "test_df= test_df.withColumn('Year', split_date.getItem(0))\n",
    "test_df= test_df.withColumn('Month', split_date.getItem(1))\n",
    "test_df= test_df.withColumn('Day', split_date.getItem(2))\n",
    "test_df = test_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"Day\", test_df[\"Day\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"Month\", test_df[\"Month\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"Year\", test_df[\"Year\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Day\", train_df[\"Day\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Month\", train_df[\"Month\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Year\", train_df[\"Year\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onehotencoded Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of categorical columns which are:\n",
    "* onpromotion\n",
    "* state\n",
    "* store_type\n",
    "* family\n",
    "* city\n",
    "* holiday_type\n",
    "* locale\n",
    "* locale_name\n",
    "\n",
    "As all the above mentioned columns were valuable for unit sales prediction, therefore we converted them into Dummy variables using StringIndexer and OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the train data into another variable\n",
    "train_df1 = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy variable creation for train data\n",
    "inputcolumns = [\"onpromotion\",\"state\",\"store_type\",\"family\",\"city\",\"holiday_type\",\"locale\",\"locale_name\"]\n",
    "indexer = [StringIndexer(stringOrderType = 'alphabetAsc', inputCol = col, outputCol = \"{0}_index\".format(col)) for col in inputcolumns]\n",
    "encoder = [OneHotEncoder(inputCol = idx.getOutputCol(), outputCol = \"{0}_feat\".format(idx.getOutputCol()),dropLast = False) for idx in indexer]\n",
    "assembler = VectorAssembler(inputCols=['onpromotion_index_feat','state_index_feat','store_type_index_feat'\\\n",
    "                                      ,'family_index_feat','city_index_feat','holiday_type_index_feat','locale_index_feat','locale_name_index_feat'\\\n",
    "                                      ,'store_nbr', 'item_nbr','cluster','class','perishable','Month','Day','transactions']\\\n",
    "                            ,outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=indexer + encoder + [assembler])\n",
    "model = pipeline.fit(train_df1)\n",
    "transformed = model.transform(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy variable creation for test data\n",
    "inputcolumns = [\"onpromotion\",\"state\",\"store_type\",\"family\",\"city\",\"holiday_type\",\"locale\",\"locale_name\"]\n",
    "testindexer = [StringIndexer(stringOrderType = 'alphabetAsc', inputCol = col, outputCol = \"{0}_index\".format(col)) for col in inputcolumns]\n",
    "testencoder = [OneHotEncoder(inputCol = idx.getOutputCol(), outputCol = \"{0}_feat\".format(idx.getOutputCol()),dropLast = False) for idx in indexer]\n",
    "testassembler = VectorAssembler(inputCols=['onpromotion_index_feat','state_index_feat','store_type_index_feat'\\\n",
    "                                      ,'family_index_feat','city_index_feat','holiday_type_index_feat','locale_index_feat','locale_name_index_feat'\\\n",
    "                                      ,'store_nbr', 'item_nbr','cluster','class','perishable','Month','Day','transactions']\\\n",
    "                            ,outputCol=\"features\")\n",
    "testpipeline = Pipeline(stages=indexer + encoder + [assembler])\n",
    "testmodel = pipeline.fit(test_df)\n",
    "testtransformed = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping categorical columns and indexed columns thereby keeping only the numerical columns and dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.drop('onpromotion','onpromotion_index','state','state_index','city','city_index',\\\n",
    "                               'store_type','store_type_index','family','family_index','holiday_type','holiday_type_index',\\\n",
    "                               'locale','locale_index','locale_name','locale_name_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtransformed = testtransformed.drop('onpromotion','onpromotion_index','state','state_index','city','city_index',\\\n",
    "                               'store_type','store_type_index','family','family_index','holiday_type','holiday_type_index',\\\n",
    "                               'locale','locale_index','locale_name','locale_name_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Code for performing PCA on the dataset. As the scree plot for PCA was unclear and almost even. We decided to not perform PCA on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assembler2 = feature.VectorAssembler(inputCols = ['store_nbr','item_nbr','cluster','class','perishable'\\\n",
    "# #                                                  ,'transactions','Month','Day','features'],outputCol='pcafeatures')\n",
    "# std_scaled2 = feature.StandardScaler(inputCol='features', outputCol='standardizedFeatures')\n",
    "# pca3 = feature.PCA(k=17, inputCol='standardizedFeatures', outputCol='pc')\n",
    "# dpipe_pca = Pipeline(stages = [std_scaled2, pca3]).fit(training_df)\n",
    "# dtrain = dpipe_pca.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_model = dpipe_pca.stages[-1]\n",
    "# pc1 = np.absolute(pca_model.pc.toArray()[:, 0]).tolist()\n",
    "# pc2 =np.absolute(pca_model.pc.toArray()[:, 1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainedVariance = dpipe_pca.stages[-1].explainedVariance\n",
    "# explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# y = explainedVariance.toArray().tolist()\n",
    "# y = np.sort(np.cumsum(y))/np.sum(y)\n",
    "# plt.plot(y)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('Explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peforming Linear Regression on the dataset containing all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have a spearate test dataset we split the train into training and validation\n",
    "lineartrainingdf, linearvalidationdf = transformed.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving columns\n",
    "pcacols = [x for x in lineartrainingdf.columns]\n",
    "pcacols.pop(2)\n",
    "pcacols.pop(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcols = pcacols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.select(fn.max(\"unit_sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.select(fn.min(\"unit_sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Model 1 pipeline and fitting it on training data\n",
    "lrmodel1 = Pipeline(stages=[\n",
    "#     feature.VectorAssembler(inputCols = pcacols, outputCol = 'features'),\n",
    "    regression.LinearRegression(featuresCol='features', labelCol='unit_sales')  \n",
    "]).fit(lineartrainingdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lrmodel1.transform(linearvalidationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = fn.sqrt(fn.mean((fn.col('unit_sales') - fn.col('prediction'))**2)).alias('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating RMSE of Model1\n",
    "lrmodel1.transform(linearvalidationdf).select(rmse).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retreiving Coefficients\n",
    "coef = lrmodel1.stages[-1].coefficients.toArray()\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriving the labels of the OneHotEncoded columns to better represent the coefficients\n",
    "sales_col_list = ['item_nbr','store_nbr','cluster','class','perishable','Month','Day']\n",
    "\n",
    "l1 = model.stages[0].labels\n",
    "for i in l1:\n",
    "    label1 = i.replace(' ','_')\n",
    "    sales_col_list.append('onpromotion_'+ label1)\n",
    "    \n",
    "l2 = model.stages[1].labels\n",
    "for i in l2:\n",
    "    label2 = i.replace(' ','_')\n",
    "    sales_col_list.append('state_'+ label2)\n",
    "    \n",
    "l3 = model.stages[2].labels\n",
    "for i in l3:\n",
    "    label3 = i.replace(' ','_')\n",
    "    sales_col_list.append('store_type_'+ label3)\n",
    "    \n",
    "l4 = model.stages[3].labels\n",
    "for i in l4:\n",
    "    label4 = i.replace(' ','_')\n",
    "    sales_col_list.append('family_'+ label4)\n",
    "    \n",
    "l5 = model.stages[4].labels\n",
    "for i in l5:\n",
    "    label5 = i.replace(' ','_')\n",
    "    sales_col_list.append('holiday_type_'+ label5)\n",
    "    \n",
    "l6 = model.stages[5].labels\n",
    "for i in l6:\n",
    "    label6= i.replace(' ','_')\n",
    "    sales_col_list.append('locale_'+ label6)\n",
    "    \n",
    "l7 = model.stages[6].labels\n",
    "for i in l7:\n",
    "    label7= i.replace(' ','_')\n",
    "    sales_col_list.append('locale_name_'+ label7)\n",
    "    \n",
    "l8 = model.stages[7].labels\n",
    "for i in l8:\n",
    "    label8 = i.replace(' ','_')\n",
    "    sales_col_list.append('city_'+ label8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "feature_importance = pd.DataFrame(list(zip(sales_col_list, coef)),\\\n",
    "                                 columns = ['feature', 'Coefficient']).sort_values('Coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance.loc[feature_importance['Coefficient']>0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Random Forest on the dataset containing all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df\n",
    "transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rftraining_df, rfvalidation_df = transformed.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Model 2 pipeline\n",
    "RF_pipe = Pipeline(stages = [regression.RandomForestRegressor(featuresCol='features', labelCol='unit_sales')]).fit(rftraining_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.RegressionEvaluator(labelCol='unit_sales', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating RMSE of Model 2\n",
    "rmseRF=evaluator.evaluate(RF_pipe.transform(rfvalidation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmseRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Year column\n",
    "pcacols.pop(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "imp = RF_pipe.stages[-1].featureImportances.toArray()\n",
    "feature_importance = pd.DataFrame(list(zip(pcacols, imp)),\\\n",
    "                                 columns = ['feature', 'importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Actual Sales vs Predicted Sales plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = a.groupBy('Month').agg(fn.sum('unit_sales').alias('Unit_Sales'),fn.sum('prediction').alias('Predicted_Sales')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Month\",y=\"Unit_Sales\", data = x1)\n",
    "sns.lineplot(x=\"Month\",y=\"Predicted_Sales\", data = x1)\n",
    "plt.legend(labels=['Actual Sales','Predicted Sales'])\n",
    "plt.ylabel(\"Sum of Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Linear Regression with Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Regression the important features which were retrived from the feature importance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate = transformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\",\"unit_sales\")\n",
    "moderate = moderate.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\",\"class\",\"family_index_feat\").agg(fn.sum(\"unit_sales\"))\n",
    "lineartrainingdf2, linearvalidationdf2 = moderate.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineartrainingdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodcols = ['item_nbr','Month','Day','perishable','class','family_index_feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Model3 pipeline\n",
    "lrmodel2 = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols = lrmodcols, outputCol = 'features'),\n",
    "    regression.LinearRegression(featuresCol='features', labelCol='sum(unit_sales)')  \n",
    "]).fit(lineartrainingdf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel2.transform(linearvalidationdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse2 = fn.sqrt(fn.mean((fn.col('sum(unit_sales)') - fn.col('prediction'))**2)).alias('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating RMSE of Model 3\n",
    "lrmodel2.transform(linearvalidationdf2).select(rmse2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Random Forest with Important Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Random Forest on Important Features retrieved from Feature Importance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderatef = transformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"unit_sales\")\n",
    "moderatef = moderatef.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\").agg(fn.sum(\"unit_sales\"))\n",
    "randomtrainingdf2, randomvalidationdf2 = moderatef.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodcols.pop(4)\n",
    "lrmodcols.pop(4)\n",
    "# lrmodcols.pop(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_modpipe = Pipeline(stages = [feature.VectorAssembler(inputCols = lrmodcols, outputCol = 'features'),\\\n",
    "                                regression.RandomForestRegressor(featuresCol='features', labelCol='sum(unit_sales)')]).fit(randomtrainingdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.RegressionEvaluator(labelCol='sum(unit_sales)', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating RMSE of Model 4\n",
    "rmseRF=evaluator.evaluate(RF_modpipe.transform(randomvalidationdf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmseRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RF_modpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Random Forest with Important features had less RMSE than another other Model. We are performing Prediction based on the Model 4 as the final model.\n",
    "\n",
    "We have created 2 functions below for predicting sales:\n",
    "1) Predict Unit Sales by Month and Day\n",
    "2) Predict Unit Sales by Month and Item Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.transform(randomvalidationdf2).groupby('Month','Day').agg(fn.avg('sum(unit_sales)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Predicting Sales by Month and Day\n",
    "def PredictSalesDay(M, D):\n",
    "    abc = randomtrainingdf2.filter(fn.col('Month') == M)\n",
    "    abc = abc.filter(fn.col('Day') == D)\n",
    "    data = final_model.transform(abc)\n",
    "    sales = data.select(fn.avg('prediction')).alias(\"Predicted Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSalesDay(1,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmod = testtransformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\",\"unit_sales\")\n",
    "# testmod = testmod.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\",\"class\",\"family_index_feat\").agg(fn.sum(\"unit_sales\"))\n",
    "# randomtrainingdf2, randomvalidationdf2 = moderatef.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model.transform(randomvalidationdf2).groupby('Month','item_nbr').agg(fn.sum('sum(unit_sales)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmodel = Pipeline(stages=[\n",
    "#     feature.VectorAssembler(inputCols = pcacols, outputCol = 'features'),\n",
    "#     regression.RandomForestRegressor(featuresCol='features', labelCol='sum(unit_sales)')  \n",
    "# ]).fit(testtransformed)\n",
    "final_model = RF_modpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmoderatef = testtransformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting Sales by Month and Item number\n",
    "def PredictSales(M, I):\n",
    "    abc = randomtrainingdf2.filter(fn.col('Month') == M)\n",
    "    abc = abc.filter(fn.col('item_nbr') == I)\n",
    "    data = final_model.transform(abc)\n",
    "#     data = \n",
    "    sales = data.select(fn.sum('prediction')).alias(\"Predicted Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSales(12,2130553)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was suggested by Professor during the Poster Presentation, we created a dataframe containing sales prediction for the provided item number, Month, Day and perishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd = spark.createDataFrame([(623316,12,10,1)],['item_nbr','Month','Day','perishable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_model.transform(abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtransformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
